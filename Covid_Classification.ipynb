{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Covid Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "17HmWUz07hRIvmPX_MHH8abD2HCEdlsWf",
      "authorship_tag": "ABX9TyMNbEF/C3e4fOF4loBGpsU3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emreer94/COVID-19_binary_classifier/blob/main/Covid_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOB2515GpCMb"
      },
      "source": [
        "# Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3gEaPgmoVXF"
      },
      "source": [
        "from keras.layers import Input, Lambda, Dense, Flatten\n",
        "from keras.models import Model\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# re-size all the images to this, default input size for VGG16\n",
        "IMAGE_SIZE = [224, 224]\n",
        "\n",
        "train_path = '/content/drive/MyDrive/nonaugmented_dataset/train'\n",
        "valid_path = '/content/drive/MyDrive/nonaugmented_dataset/test'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOCvit0GpUQV"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9i22VIV_pioM"
      },
      "source": [
        "# add preprocessing layer to the front of VGG\n",
        "vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
        "\n",
        "# don't train existing weights\n",
        "for layer in vgg.layers:\n",
        "  layer.trainable = False\n",
        "  \n",
        "\n",
        "  \n",
        "  # useful for getting number of classes\n",
        "folders = glob('/content/drive/MyDrive/nonaugmented_dataset/*')\n",
        "print(len(folders))\n",
        "\n",
        "# our layers - you can add more if you want\n",
        "x = Flatten()(vgg.output)\n",
        "# x = Dense(1000, activation='relu')(x)\n",
        "prediction = Dense(len(folders), activation='softmax')(x)\n",
        "\n",
        "# create a model object\n",
        "model = Model(inputs=vgg.input, outputs=prediction)\n",
        "\n",
        "# view the structure of the model\n",
        "model.summary()\n",
        "\n",
        "# dictate the model what cost and optimization method will be used\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEZUhvqAppy9"
      },
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwNcQOvKptZg"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator #Generates batches of tensor image data with augmentation\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "training_set = train_datagen.flow_from_directory('/content/drive/MyDrive/nonaugmented_dataset/train',\n",
        "                                                 target_size = (224, 224),\n",
        "                                                 batch_size = 32,\n",
        "                                                 classes = ('normal','covid'),\n",
        "                                                 class_mode = 'categorical')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('/content/drive/MyDrive/nonaugmented_dataset/test',\n",
        "                                            target_size = (224, 224),\n",
        "                                            batch_size = 32,\n",
        "                                            classes = ('normal','covid'),\n",
        "                                            class_mode = 'categorical')\n",
        "\n",
        "'''r=model.fit_generator(training_set,\n",
        "                         samples_per_epoch = 2000,\n",
        "                         nb_epoch = 20,\n",
        "                         validation_data = test_set,\n",
        "                         nb_val_samples = 2000)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_vP4QSOpwg4"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC2N_098p0tp"
      },
      "source": [
        "# fit the model\n",
        "r = model.fit(\n",
        "  training_set,\n",
        "  validation_data=test_set,\n",
        "  epochs=20,\n",
        "  steps_per_epoch=len(training_set),\n",
        "  validation_steps=len(test_set)\n",
        ")\n",
        "# loss\n",
        "plt.plot(r.history['loss'], label='train loss')\n",
        "plt.plot(r.history['val_loss'], label='val loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('LossVal_loss')\n",
        "\n",
        "# accuracy\n",
        "plt.plot(r.history['accuracy'], label='train accuracy')\n",
        "plt.plot(r.history['val_accuracy'], label='val accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('AccVal_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}